{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pycrfsuite\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from string import punctuation\n",
    "\n",
    "import re\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is reimplementation of the original NYT Ingredient Phrase tagger(https://github.com/NYTimes/ingredient-phrase-tagger).\n",
    "\n",
    "as you will see, the NYT's implementation uses CRF++ extractor to extract training data(You can see the readme)\n",
    "\n",
    "I used the same for extracting the training data as accepted by CRF++.\n",
    "But hear, the pycrfsuite accepts data in the form of continous list.\n",
    "So we have to convert that input-data into a format suitable for prfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you can see the readme in the above link about how to generate the text file\n",
    "with open('train_file') as fname:\n",
    "    lines = fname.readlines()\n",
    "    items = [line.strip('\\n').split('\\t') for line in lines]\n",
    "    items = [item for item in items if len(item)==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Small', 'I1', 'L12', 'YesCAP', 'NoPAREN', 'B-COMMENT'],\n",
       " ['amount', 'I2', 'L12', 'NoCAP', 'NoPAREN', 'I-COMMENT'],\n",
       " ['of', 'I3', 'L12', 'NoCAP', 'NoPAREN', 'I-COMMENT'],\n",
       " ['butter', 'I4', 'L12', 'NoCAP', 'NoPAREN', 'B-NAME'],\n",
       " ['and', 'I5', 'L12', 'NoCAP', 'NoPAREN', 'I-NAME'],\n",
       " ['flour', 'I6', 'L12', 'NoCAP', 'NoPAREN', 'I-NAME'],\n",
       " ['for', 'I7', 'L12', 'NoCAP', 'NoPAREN', 'B-COMMENT'],\n",
       " ['preparing', 'I8', 'L12', 'NoCAP', 'NoPAREN', 'I-COMMENT'],\n",
       " ['baking', 'I9', 'L12', 'NoCAP', 'NoPAREN', 'I-COMMENT'],\n",
       " ['sheet', 'I10', 'L12', 'NoCAP', 'NoPAREN', 'I-COMMENT']]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PyCRFSuite expects a list of input sequences. \n",
    "#So we process the items from the train file \n",
    "#and bucket them into sentences\n",
    "\n",
    "sentences = []\n",
    "\n",
    "sent = [items[0]]\n",
    "for item in items[1:]:\n",
    "    if 'I1' in item:\n",
    "        sentences.append(sent)\n",
    "        sent = [item]\n",
    "    else:\n",
    "        sent.append(item)\n",
    "\n",
    "sentences = sentences[:50000]\n",
    "\n",
    "#randomly shuffling the data is a good practice  before training\n",
    "random.shuffle(sentences)\n",
    "\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['6', 'I1', 'L8', 'NoCAP', 'NoPAREN'],\n",
       "  ['unpeeled', 'I2', 'L8', 'NoCAP', 'NoPAREN'],\n",
       "  ['garlic', 'I3', 'L8', 'NoCAP', 'NoPAREN'],\n",
       "  ['cloves', 'I4', 'L8', 'NoCAP', 'NoPAREN']],\n",
       " ['B-QTY', 'B-COMMENT', 'B-NAME', 'B-UNIT'])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split_value = int(0.1 * len(sentences))\n",
    "\n",
    "test_data = sentences[:train_test_split_value]\n",
    "train_data = sentences[train_test_split_value:]\n",
    "\n",
    "#we will sepatate labels, features and tokens from the data\n",
    "def create_labels(sent):\n",
    "    return [word[-1] for word in sent]\n",
    "\n",
    "def create_features(sent):\n",
    "    return [word[:-1] for word in sent]\n",
    "\n",
    "def create_tokens(sent):\n",
    "    return [word[0] for word in sent]   \n",
    "\n",
    "y_train = [create_labels(s) for s in train_data]\n",
    "X_train = [create_features(s) for s in train_data]\n",
    "\n",
    "X_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOw we define The trainer class.\n",
    "# This class maintains a data set for training, and provides an interface to various training algorithms.\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "# Append an instance (item/label sequence) to the data set.\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "\n",
    "#this are the tuned hyperparameters for the training which I found from the internet because of lack of time\n",
    "trainer.set_params(\n",
    "{\n",
    "        'c1': 0.43,\n",
    "        'c2': 0.012,\n",
    "        'max_iterations': 100,\n",
    "        'feature.possible_transitions': True,\n",
    "        'feature.possible_states': True,\n",
    "        'linesearch': 'StrongBacktracking'\n",
    "    }\n",
    ")\n",
    "\n",
    "#Run the training algorithm. This function starts the training \n",
    "# and saves the trained model to \"trained_pycrfsuite\"\n",
    "#so that we don't have to train it again as the training process is slow\n",
    "#depending on the data size\n",
    "trainer.train('trained_pycrfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7fea26384390>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the trained model\n",
    "#to do this we use taggers\n",
    "\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('trained_pycrfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "def get_sentence_features(sent):\n",
    "#     Gets  the features of the sentence\n",
    "    sent_tokens = utils.tokenize(utils.cleanUnicodeFractions(sent))\n",
    "\n",
    "    sent_features = []\n",
    "    for i, token in enumerate(list(sent_tokens)):\n",
    "        token_features = [token]\n",
    "        token_features.extend(utils.getFeatures(token, i+1, list(sent_tokens)))\n",
    "        sent_features.append(token_features)\n",
    "    return sent_features\n",
    "\n",
    "def format_ingredient_output(tagger_output, display=False):\n",
    "#     \"\"\"Formats the tagger output into a more convenient dictionary\"\"\"\n",
    "    data = [{}]\n",
    "    display = [[]]\n",
    "    prevTag = None\n",
    "\n",
    "\n",
    "    for token, tag in tagger_output:\n",
    "    # turn B-NAME/123 back into \"name\"\n",
    "        tag = re.sub(r'^[BI]\\-', \"\", tag).lower()\n",
    "\n",
    "        # ---- DISPLAY ----\n",
    "        # build a structure which groups each token by its tag, so we can\n",
    "        # rebuild the original display name later.\n",
    "\n",
    "        if prevTag != tag:\n",
    "            display[-1].append((tag, [token]))\n",
    "            prevTag = tag\n",
    "        else:\n",
    "            display[-1][-1][1].append(token)\n",
    "            #               ^- token\n",
    "            #            ^---- tag\n",
    "            #        ^-------- ingredient\n",
    "\n",
    "            # ---- DATA ----\n",
    "            # build a dict grouping tokens by their tag\n",
    "\n",
    "            # initialize this attribute if this is the first token of its kind\n",
    "        if tag not in data[-1]:\n",
    "            data[-1][tag] = []\n",
    "\n",
    "        # HACK: If this token is a unit, singularize it so Scoop accepts it.\n",
    "        if tag == \"unit\":\n",
    "            token = utils.singularize(token)\n",
    "\n",
    "        data[-1][tag].append(token)\n",
    "\n",
    "    # reassemble the output into a list of dicts.\n",
    "    output = [\n",
    "        dict([(k, utils.smartJoin(tokens)) for k, tokens in ingredient.items()])\n",
    "        for ingredient in data\n",
    "        if len(ingredient)\n",
    "    ]\n",
    "\n",
    "    # Add the raw ingredient phrase\n",
    "    for i, v in enumerate(output):\n",
    "        output[i][\"input\"] = utils.smartJoin(\n",
    "            [\" \".join(tokens) for k, tokens in display[i]])\n",
    "\n",
    "    return output\n",
    "\n",
    "def parse_ingredient(sent):\n",
    "#     \"\"\"ingredient parsing logic\"\"\"\n",
    "    sentence_features = get_sentence_features(sent)\n",
    "    tags = tagger.tag(sentence_features)\n",
    "    tagger_output = zip(create_tokens(sentence_features), tags)\n",
    "    parsed_ingredient =  format_ingredient_output(tagger_output)\n",
    "    if parsed_ingredient:\n",
    "        parsed_ingredient[0]['name'] = parsed_ingredient[0].get('name','').strip('.')\n",
    "    return parsed_ingredient\n",
    "\n",
    "def parse_recipe_ingredients(ingredient_list):\n",
    "\n",
    "#     \"\"\"Wrapper around parse_ingredient so we can call it on an ingredient list\"\"\"\n",
    "    sentences = tokenizer.tokenize(q)\n",
    "    \n",
    "    sentences = [sent.strip('\\n') for sent in sentences]\n",
    "    ingredients = []\n",
    "    for sent in sentences:\n",
    "        ingredients.extend(parse_ingredient(sent))\n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': '2$1/4 cups all-purpose flour.',\n",
       "  'name': 'all-purpose flour',\n",
       "  'qty': '2$1/4',\n",
       "  'unit': 'cup'},\n",
       " {'input': '1/2 teaspoon baking soda.',\n",
       "  'name': 'baking soda',\n",
       "  'qty': '1/2',\n",
       "  'unit': 'teaspoon'},\n",
       " {'comment': '(2 sticks) room temperature.',\n",
       "  'input': '1 cup (2 sticks) unsalted butter, room temperature.',\n",
       "  'name': 'unsalted butter',\n",
       "  'other': ',',\n",
       "  'qty': '1',\n",
       "  'unit': 'cup'},\n",
       " {'comment': 'granulated',\n",
       "  'input': '1/2 cup granulated sugar.',\n",
       "  'name': 'sugar',\n",
       "  'qty': '1/2',\n",
       "  'unit': 'cup'},\n",
       " {'input': '1 cup packed light-brown sugar.',\n",
       "  'name': 'packed light-brown sugar',\n",
       "  'qty': '1',\n",
       "  'unit': 'cup'},\n",
       " {'input': '1 teaspoon salt.', 'name': 'salt', 'qty': '1', 'unit': 'teaspoon'},\n",
       " {'comment': 'pure',\n",
       "  'input': '2 teaspoons pure vanilla extract.',\n",
       "  'name': 'vanilla extract',\n",
       "  'qty': '2',\n",
       "  'unit': 'teaspoon'},\n",
       " {'comment': 'large', 'input': '2 large eggs.', 'name': 'eggs', 'qty': '2'},\n",
       " {'comment': '(about) semisweet and/or',\n",
       "  'input': '2 cups (about 12 ounces) semisweet and/or milk chocolate chips.',\n",
       "  'name': 'milk chocolate chips',\n",
       "  'qty': '2 12',\n",
       "  'unit': 'cup ounce'}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will pass some examples and check whether the code we wrote is working perfectly\n",
    "# you can see from the output that its extracting the data accurately\n",
    "\n",
    "q = '''\n",
    "2 1/4 cups all-purpose flour.\n",
    "1/2 teaspoon baking soda.\n",
    "1 cup (2 sticks) unsalted butter, room temperature.\n",
    "1/2 cup granulated sugar.\n",
    "1 cup packed light-brown sugar.\n",
    "1 teaspoon salt.\n",
    "2 teaspoons pure vanilla extract.\n",
    "2 large eggs.\n",
    "2 cups (about 12 ounces) semisweet and/or milk chocolate chips.\n",
    "'''\n",
    "\n",
    "parse_recipe_ingredients(q)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# '''\n",
    "# The output is in the format\n",
    "# [{ 'comment': extra comments #if available#\n",
    "#   'input': input we are passing to the extractor\n",
    "#   'name': name of the item ordered\n",
    "#   'qty': quantity required of the ordered item\n",
    "#   'unit': unit\n",
    "#  }\n",
    "\n",
    "# ]\n",
    "# '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
